{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pramo92/Deep-Learning-Mini-Project-03/blob/main/English_to_Sinhala_Translation_with_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AubAIrrZlvSd"
      },
      "source": [
        "#English to Sinhala Translation with Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WARVy8Tzl1wt"
      },
      "source": [
        "##Necessary Library Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "VfX4Tb8RltT0"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yThatquGHjji"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_Ca4HqWl8X9"
      },
      "source": [
        "##Prepare the Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xeiFawKl_32"
      },
      "source": [
        "###Mount the Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Svjoo7jjmCy8",
        "outputId": "ef8e8f47-2c2a-4eb0-8b1b-8a1bbe1a656d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBM_NHLkmHep"
      },
      "source": [
        "###Read the data file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55PhFfeWmSKw",
        "outputId": "54be2042-9f37-44e2-9d4d-30ddac972e5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tයන්න.\n",
            "Hi.\tආයුබෝවන්.\n",
            "Run.\tදුවන්න.\n",
            "Who?\tකවුද?\n",
            "Wow!\tවාව්!\n",
            "Fire!\tගිනි!\n",
            "Help!\tඋදව්!\n",
            "Hide.   සඟවන්න.\t\n",
            "Jump.   පනින්න.\t\n",
            "Stop!   නවත්වන්න!\t\n",
            "Wait!   ඉන්න!\t\n",
            "Wait.   ඉන්න.\t\n",
            "Begin.  ආරම්භය.\t\n",
            "Go on.  යන්න.\t\t\n",
            "Hello.\tආයුබෝවන්.\n",
            "I ran.\tමම දිව්වා.\n",
            "I try.\tමම උත්සාහ කරනවා.\n",
            "I won!\tමම දිනුවා!\n",
            "Oh no!\tඅපොයි නෑ!\n",
            "Relax.\tසන්සුන් වන්න.\n"
          ]
        }
      ],
      "source": [
        "text_file = \"/content/drive/My Drive/dataset1\"\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "i = 0\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  i = i + 1\n",
        "  if(i==20):\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2CPKz5CnXkZ",
        "outputId": "31e43d21-9976-4e98-fd1c-ecfb02f99014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Death is something that we're often discouraged to talk about or even think about, but I've realized that preparing for death is one of the most empowering things you can do. Thinking about death clarifies your life.    මරණය යනු අප බොහෝ විට කතා කිරීමට හෝ ඒ ගැන සිතීමට අධෛර්යමත් කරන දෙයකි, නමුත් මරණයට සූදානම් වීම ඔබට කළ හැකි වඩාත්ම බලගතු දෙයක් බව මම තේරුම් ගතිමි. මරණය ගැන සිතීම ඔබේ ජීවිතය පැහැදිලි කරයි.\t\n",
            "Since there are usually multiple websites on any given topic, I usually just click the back button when I arrive on any webpage that has pop-up advertising. I just go to the next page found by Google and hope for something less irritating.  ඕනෑම මාතෘකාවක් මත සාමාන්‍යයෙන් වෙබ් අඩවි කිහිපයක් ඇති බැවින්, මම සාමාන්‍යයෙන් උත්පතන වෙළඳ දැන්වීම් ඇති ඕනෑම වෙබ් පිටුවකට පැමිණෙන විට ආපසු බොත්තම ක්ලික් කරන්න. මම Google විසින් සොයා ගන්නා ලද ඊළඟ පිටුවට ගොස් අඩු කුපිත කරවන දෙයක් බලාපොරොත්තු වෙමි.\t\n",
            "If someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.    ඔබේ පසුබිම නොදන්නා කෙනෙක් ඔබ ස්වදේශික කථිකයෙකු ලෙස ශබ්ද කරන බව පැවසුවහොත්, එයින් අදහස් වන්නේ ඔබ ස්වදේශික කථිකයෙකු නොවන බව ඔවුන්ට වැටහෙන ඔබේ කතා කිරීමේ යම් දෙයක් ඔවුන් දැක ඇති බවයි. වෙනත් වචන වලින් කිවහොත්, ඔබ ඇත්තටම ස්වදේශික කථිකයෙකු ලෙස නොපෙනේ.\t\n",
            "It may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort. However, if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning, we might be able to minimize errors. මේ ආකාරයේ සහයෝගී උත්සාහයක ස්වභාවය නිසා සම්පූර්ණ දෝෂ රහිත corpus එකක් ලබා ගැනීමට නොහැකි විය හැකිය. කෙසේ වෙතත්, අපි සාමාජිකයින්ට ඔවුන් ඉගෙන ගන්නා භාෂා අත්හදා බැලීමට වඩා ඔවුන්ගේම භාෂාවෙන් වාක්‍ය දායක කිරීමට දිරිගන්වන්නේ නම්, අපට දෝෂ අවම කර ගැනීමට හැකි වනු ඇත.\t\n",
            "I went drinking with one of my boyfriend's friends, and now he's furious at me. \"Was this friend a guy or a girl?\" \"A guy, obviously. Why would I go drinking with his female friends?\" \"Yeah, you're right.\" \"His name is Tom. He's really hot, and I really want to go drinking with him again.\"   මම මගේ පෙම්වතාගේ යාළුවෙක් එක්ක බොන්න ගියා, දැන් එයා මාත් එක්ක තරහයි. \"මේ යාළුවා කොල්ලෙක්ද කෙල්ලෙක්ද?\" \"පිරිමියෙක්, පැහැදිලිවම. ඇයි මම ඔහුගේ ගැහැණු මිතුරන් සමඟ බොන්න යන්නේ?\" \"ඔව්, ඔබ නිවැරදි.\" \"ඔහුගේ නම ටොම්. ඔහු ඇත්තටම උණුසුම්, මට ඇත්තටම ඔහු සමඟ නැවත මත්පැන් පානය කිරීමට අවශ්‍යයි.\"\t\n",
            "If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\tඔබට ස්වදේශික කථිකයෙකු ලෙස ශබ්ද කිරීමට අවශ්‍ය නම්, බැන්ජෝ ක්‍රීඩකයින් එම වාක්‍ය ඛණ්ඩය නිවැරදිව හා අපේක්ෂිත වේගයට වාදනය කිරීමට හැකි වන තෙක් එකම වාක්‍ය ඛණ්ඩය නැවත නැවතත් පුහුණු කරන ආකාරයටම එම වාක්‍යය නැවත නැවතත් කීමට ඔබ කැමති විය යුතුය.\n",
            "There are mothers and fathers who will lie awake after the children fall asleep and wonder how they'll make the mortgage, or pay their doctor's bills, or save enough for their child's college education.\tදරුවන් නිදාගත් පසු අවදියෙන් සිටින මව්වරුන් සහ පියවරුන් ඔවුන් උකස් කරන්නේ කෙසේද, හෝ වෛද්‍යවරයාගේ බිල්පත් ගෙවන්නේ කෙසේද, නැතහොත් තම දරුවාගේ විශ්ව විද්‍යාල අධ්‍යාපනයට ප්‍රමාණවත් මුදලක් ඉතිරි කරන්නේ කෙසේදැයි කල්පනා කරන මව්වරුන් සහ පියවරුන් සිටිති.\n",
            "In 1969, Roger Miller recorded a song called \"You Don't Want My Love.\" Today, this song is better known as \"In the Summer Time.\" It's the first song he wrote and sang that became popular.\t1969 දී රොජර් මිලර් \"ඔබට මගේ ආදරය අවශ්‍ය නැත\" නමින් ගීතයක් පටිගත කළේය. අද මෙම ගීතය වඩාත් ප්‍රචලිත වන්නේ \"ගිම්හාන කාලයේ\" යනුවෙනි. එය ඔහු ලියූ සහ ගායනා කළ පළමු ගීතය ජනප්‍රිය විය.\n",
            "A mistake young people often make is to start learning too many languages at the same time, as they underestimate the difficulties and overestimate their own ability to learn them.\tයෞවනයන් බොහෝ විට කරන වැරැද්දක් නම්, ඔවුන් දුෂ්කරතා අවතක්සේරු කිරීම සහ ඒවා ඉගෙන ගැනීමට ඇති හැකියාව අධිතක්සේරු කිරීම නිසා එකවර බොහෝ භාෂා ඉගෙන ගැනීමට පටන් ගැනීමයි.\n",
            "You can't view Flash content on an iPad. However, you can easily email yourself the URLs of these web pages and view that content on your regular computer when you get home.\tඔබට iPad එකක Flash අන්තර්ගතය බැලිය නොහැක. කෙසේ වෙතත්, ඔබට මෙම වෙබ් පිටු වල URL ඔබට පහසුවෙන් විද්‍යුත් තැපෑලෙන් යැවිය හැකි අතර ඔබ නිවසට පැමිණි පසු එම අන්තර්ගතය ඔබේ සාමාන්‍ය පරිගණකයෙන් බැලිය හැක.\n"
          ]
        }
      ],
      "source": [
        "for x in range(len(lines)-10,len(lines)):\n",
        "  print(lines[x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AZ-RaEInm-d"
      },
      "source": [
        "###Split the English and Spanish translation *pairs*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0vUyK6Rnl67",
        "outputId": "086c1082-bf11-4ba1-d10f-a9a918b238b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('He told his men they would attack the next night.', '[start] ඔහු තම මිනිසුන්ට පැවසුවේ ඔවුන් ඊළඟ රාත්\\u200dරියේ පහර දෙන බවයි. [end]')\n",
            "('A thick hedge surrounded the garden.', '[start] වත්ත වටේ ඝන වැටක්. [end]')\n",
            "('I have a friend whose father is a magician.', '[start] මගේ යාළුවෙක් ඉන්නවා එයාගේ තාත්තා මැජික් කාරයෙක්. [end]')\n"
          ]
        }
      ],
      "source": [
        "text_pairs = []\n",
        "for line in lines:\n",
        "    # Check if the line contains exactly one tab character\n",
        "    if line.count(\"\\t\") == 1:\n",
        "        english, sinhala = line.split(\"\\t\", 1)  # Split only once\n",
        "        sinhala = \"[start] \" + sinhala.strip() + \" [end]\"  # strip to remove leading/trailing whitespace\n",
        "        text_pairs.append((english.strip(), sinhala))  # strip to remove leading/trailing whitespace\n",
        "\n",
        "for i in range(3):\n",
        "    print(random.choice(text_pairs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpKdw4-9pdBe"
      },
      "source": [
        "###Randomize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "DzJFhOpjpvPh"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.shuffle(text_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjsq7-Tbpqtf"
      },
      "source": [
        "###Spliting the data into training, validation and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wzgYVxDTfLP",
        "outputId": "539c2de2-9dbb-46c9-ed47-a396a622a991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences: 17992\n",
            "Training set size: 12596\n",
            "Validation set size: 2698\n",
            "Testing set size: 2698\n",
            "Total size of the dataset: 17992\n"
          ]
        }
      ],
      "source": [
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
        "print(\"Total sentences:\",len(text_pairs))\n",
        "print(\"Training set size:\",len(train_pairs))\n",
        "print(\"Validation set size:\",len(val_pairs))\n",
        "print(\"Testing set size:\",len(test_pairs))\n",
        "print(\"Total size of the dataset:\",len(train_pairs)+len(val_pairs)+len(test_pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-dqEdKQVO0V"
      },
      "source": [
        "###Removing Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KI-CvFg7Tfvo",
        "outputId": "53b89cb6-1352-43c5-ad60-50e570b5cab1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\\\\\\\\\^_`\\\\{\\\\|\\\\}\\\\~¿]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "f\"[{re.escape(strip_chars)}]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "b3KnvVf3TfyY",
        "outputId": "2553bced-f694-40f0-f18b-83b0127aec8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'8'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "f\"{3+5}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TldosUAmVR-m"
      },
      "source": [
        "###Vectorizing the English and Spanish text pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "VvgMcsS6Tf1R"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_sinhala_texts = [pair[1] for pair in train_pairs]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "rvQFimgSTf30"
      },
      "outputs": [],
      "source": [
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_sinhala_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePQLDieSVYhq"
      },
      "source": [
        "###Preparing datasets for the translation task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfDTfmjuTf5o",
        "outputId": "c07c91ed-0d6e-4ac5-a33f-537de2094c2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (52, 20)\n",
            "inputs['sinhala'].shape: (52, 20)\n",
            "targets.shape: (52, 20)\n",
            "({'english': array([[  33,   80,   51, ...,    0,    0,    0],\n",
            "       [  69,    8,  707, ...,    0,    0,    0],\n",
            "       [   4,   94,   42, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [   4,  165, 1448, ...,    0,    0,    0],\n",
            "       [ 130,   30,    3, ...,    0,    0,    0],\n",
            "       [   5,  450,  239, ...,    0,    0,    0]]), 'sinhala': array([[   2,   19, 3151, ...,    0,    0,    0],\n",
            "       [   2, 1856, 4779, ...,    0,    0,    0],\n",
            "       [   2,    6, 1269, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [   2,    4,   81, ...,    0,    0,    0],\n",
            "       [   2,    4,   64, ...,    0,    0,    0],\n",
            "       [   2,    5,   11, ...,    0,    0,    0]])}, array([[  19, 3151, 2781, ...,    0,    0,    0],\n",
            "       [1856, 4779,   49, ...,    0,    0,    0],\n",
            "       [   6, 1269,   50, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [   4,   81,  313, ...,    0,    0,    0],\n",
            "       [   4,   64, 4024, ...,    0,    0,    0],\n",
            "       [   5,   11, 4366, ...,    0,    0,    0]]))\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "def format_dataset(eng, sin):\n",
        "    eng = source_vectorization(eng)\n",
        "    sin = target_vectorization(sin)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        \"sinhala\": sin[:, :-1],\n",
        "    }, sin[:, 1:])\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, sin_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    sin_texts = list(sin_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, sin_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['sinhala'].shape: {inputs['sinhala'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")\n",
        "inputs['english'].shape: (64, 20)\n",
        "inputs['sinhala'].shape: (64, 20)\n",
        "targets.shape: (64, 20)\n",
        "print(list(train_ds.as_numpy_iterator())[50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcq-xXnzVeMp"
      },
      "source": [
        "###Transformer encoder implemented as a subclassed Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "vr_Sg0Q_Tf7g"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6s-1vzHVheJ"
      },
      "source": [
        "###The Transformer decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "bfCn7X5iTf9Q"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = mask\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMTNz3T6VmjW"
      },
      "source": [
        "###Positional Encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Eq8JbrJeTf_k"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rscmjXe4Vpco"
      },
      "source": [
        "###End-to-end Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "atecytJCTgCB"
      },
      "outputs": [],
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"sinhala\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-vYPH8CTgEQ",
        "outputId": "ac1c4d33-d108-4bec-ec42-5de483c352ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " english (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " sinhala (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " positional_embedding_2 (Po  (None, None, 256)            3845120   ['english[0][0]']             \n",
            " sitionalEmbedding)                                                                               \n",
            "                                                                                                  \n",
            " positional_embedding_3 (Po  (None, None, 256)            3845120   ['sinhala[0][0]']             \n",
            " sitionalEmbedding)                                                                               \n",
            "                                                                                                  \n",
            " transformer_encoder_1 (Tra  (None, None, 256)            3155456   ['positional_embedding_2[0][0]\n",
            " nsformerEncoder)                                                   ']                            \n",
            "                                                                                                  \n",
            " transformer_decoder_1 (Tra  (None, None, 256)            5259520   ['positional_embedding_3[0][0]\n",
            " nsformerDecoder)                                                   ',                            \n",
            "                                                                     'transformer_encoder_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, None, 256)            0         ['transformer_decoder_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, None, 15000)          3855000   ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19960216 (76.14 MB)\n",
            "Trainable params: 19960216 (76.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "transformer.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V39cXmT8VtmE"
      },
      "source": [
        "###Training the sequence-to-sequence Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EILayuJTgGO",
        "outputId": "4ac754a6-5c07-4582-808f-03901f16945d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/28\n",
            "197/197 [==============================] - 612s 3s/step - loss: 5.8207 - accuracy: 0.2963 - val_loss: 5.1850 - val_accuracy: 0.3236\n",
            "Epoch 2/28\n",
            "197/197 [==============================] - 602s 3s/step - loss: 4.8518 - accuracy: 0.3585 - val_loss: 4.7908 - val_accuracy: 0.3586\n",
            "Epoch 3/28\n",
            "197/197 [==============================] - 595s 3s/step - loss: 4.2763 - accuracy: 0.4079 - val_loss: 4.2936 - val_accuracy: 0.4180\n",
            "Epoch 4/28\n",
            "197/197 [==============================] - 595s 3s/step - loss: 3.8175 - accuracy: 0.4498 - val_loss: 4.0312 - val_accuracy: 0.4500\n",
            "Epoch 5/28\n",
            "197/197 [==============================] - 632s 3s/step - loss: 3.4409 - accuracy: 0.4855 - val_loss: 4.1645 - val_accuracy: 0.4233\n",
            "Epoch 6/28\n",
            "197/197 [==============================] - 596s 3s/step - loss: 3.1141 - accuracy: 0.5204 - val_loss: 3.7770 - val_accuracy: 0.4789\n",
            "Epoch 7/28\n",
            "197/197 [==============================] - 591s 3s/step - loss: 2.8344 - accuracy: 0.5508 - val_loss: 3.7240 - val_accuracy: 0.4836\n",
            "Epoch 8/28\n",
            "197/197 [==============================] - 588s 3s/step - loss: 2.5825 - accuracy: 0.5789 - val_loss: 3.6690 - val_accuracy: 0.4899\n",
            "Epoch 9/28\n",
            "197/197 [==============================] - 597s 3s/step - loss: 2.3633 - accuracy: 0.6055 - val_loss: 3.6526 - val_accuracy: 0.4977\n",
            "Epoch 10/28\n",
            "197/197 [==============================] - 648s 3s/step - loss: 2.1714 - accuracy: 0.6301 - val_loss: 3.6417 - val_accuracy: 0.4986\n",
            "Epoch 11/28\n",
            "197/197 [==============================] - 596s 3s/step - loss: 1.9973 - accuracy: 0.6543 - val_loss: 3.5856 - val_accuracy: 0.5114\n",
            "Epoch 12/28\n",
            "197/197 [==============================] - 589s 3s/step - loss: 1.8477 - accuracy: 0.6752 - val_loss: 3.5883 - val_accuracy: 0.5108\n",
            "Epoch 13/28\n",
            "197/197 [==============================] - 598s 3s/step - loss: 1.7172 - accuracy: 0.6952 - val_loss: 3.6175 - val_accuracy: 0.5056\n",
            "Epoch 14/28\n",
            "197/197 [==============================] - 624s 3s/step - loss: 1.6053 - accuracy: 0.7132 - val_loss: 3.6281 - val_accuracy: 0.5136\n",
            "Epoch 15/28\n",
            "197/197 [==============================] - 625s 3s/step - loss: 1.5102 - accuracy: 0.7293 - val_loss: 3.6137 - val_accuracy: 0.5135\n",
            "Epoch 16/28\n",
            "197/197 [==============================] - 583s 3s/step - loss: 1.4165 - accuracy: 0.7457 - val_loss: 3.6362 - val_accuracy: 0.5166\n",
            "Epoch 17/28\n",
            "197/197 [==============================] - 584s 3s/step - loss: 1.3401 - accuracy: 0.7609 - val_loss: 3.6426 - val_accuracy: 0.5189\n",
            "Epoch 18/28\n",
            "197/197 [==============================] - 588s 3s/step - loss: 1.2782 - accuracy: 0.7722 - val_loss: 3.6120 - val_accuracy: 0.5213\n",
            "Epoch 19/28\n",
            "197/197 [==============================] - 624s 3s/step - loss: 1.2208 - accuracy: 0.7848 - val_loss: 3.6484 - val_accuracy: 0.5234\n",
            "Epoch 20/28\n",
            "197/197 [==============================] - 622s 3s/step - loss: 1.1796 - accuracy: 0.7934 - val_loss: 3.6119 - val_accuracy: 0.5275\n",
            "Epoch 21/28\n",
            "197/197 [==============================] - 579s 3s/step - loss: 1.1373 - accuracy: 0.8013 - val_loss: 3.6503 - val_accuracy: 0.5259\n",
            "Epoch 22/28\n",
            "197/197 [==============================] - 587s 3s/step - loss: 1.1012 - accuracy: 0.8088 - val_loss: 3.6820 - val_accuracy: 0.5268\n",
            "Epoch 23/28\n",
            "197/197 [==============================] - 582s 3s/step - loss: 1.0713 - accuracy: 0.8174 - val_loss: 3.6707 - val_accuracy: 0.5262\n",
            "Epoch 24/28\n",
            "197/197 [==============================] - 621s 3s/step - loss: 1.0503 - accuracy: 0.8212 - val_loss: 3.6904 - val_accuracy: 0.5287\n",
            "Epoch 25/28\n",
            "197/197 [==============================] - 588s 3s/step - loss: 1.0309 - accuracy: 0.8270 - val_loss: 3.7374 - val_accuracy: 0.5276\n",
            "Epoch 26/28\n",
            "197/197 [==============================] - 584s 3s/step - loss: 1.0149 - accuracy: 0.8309 - val_loss: 3.7266 - val_accuracy: 0.5244\n",
            "Epoch 27/28\n",
            "197/197 [==============================] - 588s 3s/step - loss: 1.0017 - accuracy: 0.8350 - val_loss: 3.7345 - val_accuracy: 0.5308\n",
            "Epoch 28/28\n",
            "197/197 [==============================] - 590s 3s/step - loss: 0.9979 - accuracy: 0.8371 - val_loss: 3.8289 - val_accuracy: 0.5213\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7867ea48f310>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=28, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qbKXy7mnDsA"
      },
      "source": [
        "###Manual testing of the translation model with 20 new sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "sin_vocab = target_vectorization.get_vocabulary()\n",
        "sin_index_lookup = dict(zip(range(len(sin_vocab)), sin_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = sin_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm1eOdY8wVtB",
        "outputId": "b4c286fe-5c7b-4f17-9eb7-4add8a6e6459"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "Tom said that he didn't like my dress.\n",
            "[start] ටොම් කිව්වා එයා මගේ ඇඳුම් අඳින්න කැමති කියලා [end]\n",
            "-\n",
            "He tried getting close to her using every means possible.\n",
            "[start] ඔහු තම පියාගේ අසනීප කිරීමට උත්සාහ කළේ ඔහු සෑම දෙයක්ම ඉතා එයා වෙනස් විය [end]\n",
            "-\n",
            "He is not studying English now.\n",
            "[start] ඔහු දැන් ඉංග්‍රීසි ඉගෙන ගන්න පුරුදු වෙලා [end]\n",
            "-\n",
            "Gold is more precious than any other metal.\n",
            "[start] ඕනෑම එක් පුද්ගලයා ඩොලර් පහක් ඇත [end]\n",
            "-\n",
            "She's unfit for the job.\n",
            "[start] ඇය රැකියාවක් සොයයි [end]\n",
            "-\n",
            "Mother cut the cheese with a knife.\n",
            "[start] අම්මා කුස්සියේ බිම වැටී හිස තබා ගන්න [end]\n",
            "-\n",
            "Here are the documents you asked for.\n",
            "[start] මෙන්න ඔබ සිතන්නේ ඇයි ඔබ කඳු [end]\n",
            "-\n",
            "He'd like to have a coffee after work. \"I would too.\"\n",
            "[start] මම ඔබේ පවුලේ අය සමඟ වැඩ කිරීමට කැමතියි [end]\n",
            "-\n",
            "I'm sure Tom will be surprised.\n",
            "[start] ටොම් සාදයට සහභාගී වීමට මම බලාපොරොත්තු වෙනවා කියලා [end]\n",
            "-\n",
            "Quite a few people were present at the meeting yesterday.\n",
            "[start] ඊයේ රාත්‍රියේ මේ මොහොතේ වැස්ස පටන් ගත්තා [end]\n",
            "-\n",
            "Dinosaurs died out a very long time ago.\n",
            "[start] පැය ගොඩක් මීට වසර දෙකකට පමණ පෙර මිය ගියා [end]\n",
            "-\n",
            "I prefer to err on the side of caution.\n",
            "[start] මම කැමති එහෙම ඇඳගෙන ඉන්න කැමතියි [end]\n",
            "-\n",
            "It was not clear what she said.\n",
            "[start] ඇය කීවේ කුමක් දැයි ඇය කී දේ විශ්වාස කළේ නැත [end]\n",
            "-\n",
            "Call me.\n",
            "[start] ඇය මට කතා කරන්න [end]\n",
            "-\n",
            "She's well-informed, so she might know something.\n",
            "[start] ඇය කිසිවක් කියා ඇති බව ඇය දැන සිටියේ නැත [end]\n",
            "-\n",
            "Tom does that every Monday morning.\n",
            "[start] ටොම් හැමදාම උදේ නිතරම කාර්යබහුලයි [end]\n",
            "-\n",
            "Tom has already fallen asleep.\n",
            "[start] ටොම් දැනටමත් ඔහුගේ වී ඇත [end]\n",
            "-\n",
            "If my boy had not been killed in the traffic accident, he would be a college student now.\n",
            "[start] [end]\n",
            "-\n",
            "Tom is smarter than I am.\n",
            "[start] ටොම් මට වඩා මිටියි [end]\n",
            "-\n",
            "I trust him completely.\n",
            "[start] මම ඔහු සම්පූර්ණයෙන්ම විශ්වාස කරනවා [end]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtaKi9kxj6ASYX07A+uq3W",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
